# Story 1.4: Notebook Test Script (Epic 1)

## Status
DONE

## Story
**As a** Developer/Examiner,
**I want** To create a Jupytext-compatible Python script (`.py`) that demonstrates the Epic 1 pipeline,
**So that** I can interactively run and validate the Ingestion Pipeline, RAG functionality, and SGD Generation for demonstration and examination purposes.

## Acceptance Criteria
1. Script `notebooks/E01_Pipeline_Foundation.py` created (Jupytext format)
2. **Cell 1 (Docs):** Explains purpose
3. **Cell 2 (Ingest):** Runs Ingestion Pipeline on a sample doc from `data/`
4. **Cell 3 (RAG Test):** Asks a question to RAG (V1) and prints answer
5. **Cell 4 (SGD):** Runs SGD Generation script and shows sample output from `golden_dataset/sgd_benchmark.csv`

## Tasks / Subtasks
- [x] Task 1: Create Jupytext-Compatible Notebook Script Structure (AC: 1)
  - [x] Create `notebooks/E01_Pipeline_Foundation.py` file
  - [x] Add Jupytext cell markers (`# %%`) for notebook compatibility
  - [x] Add proper Python script shebang and encoding declarations
  - [x] Set up imports for all required modules (LangChain, RAGAS, pandas, etc.)

- [x] Task 2: Implement Documentation Cell (AC: 2)
  - [x] Create Cell 1 with markdown documentation using `# %% [markdown]`
  - [x] Explain the purpose of the notebook (validating Epic 1 stories)
  - [x] Document what each subsequent cell demonstrates
  - [x] Include prerequisites and setup instructions

- [x] Task 3: Implement Ingestion Pipeline Demonstration Cell (AC: 3)
  - [x] Create Cell 2 to demonstrate document ingestion
  - [x] Import ingestion logic (refactored to use in-memory Qdrant)
  - [x] Select a sample document from `data/` directory (e.g., one of the PDF files)
  - [x] Run the Ingestion Pipeline on the sample document
  - [x] Display ingestion results (number of chunks, sample chunks, etc.)
  - [x] Verify chunks are stored in Qdrant vector store (in-memory)
  - [x] Add error handling and informative output

- [x] Task 4: Implement RAG Pipeline Demonstration Cell (AC: 4)
  - [x] Create Cell 3 to test basic RAG functionality
  - [x] Import RAG pipeline components (refactored for in-memory use)
  - [x] Configure RAG chain with Naive Retrieval (Story 1.2 implementation)
  - [x] Define a test question relevant to the ingested document
  - [x] Execute RAG query and retrieve answer
  - [x] Display the question, retrieved context, and generated answer
  - [x] Add timing/performance metrics

- [x] Task 5: Implement SGD Generation Demonstration Cell (AC: 5)
  - [x] Create Cell 4 to demonstrate SGD generation
  - [x] Execute the `backend/app/scripts/generate_sgd.py` script
  - [x] Run SGD generation with small test size (3 samples to minimize API costs)
  - [x] Load the generated CSV from `golden_dataset/sgd_benchmark.csv`
  - [x] Display sample rows from the SGD (user_input, reference_contexts, reference, synthesizer_name)
  - [x] Show summary statistics (total questions, question type distribution)
  - [x] Verify CSV structure matches RAGAS 0.37+ format

- [x] Task 6: Add Environment Setup and Configuration (AC: 1, 3, 4, 5)
  - [x] Add cell for environment variable validation (OPENAI_API_KEY)
  - [x] Add cell for checking required directories exist (data/, golden_dataset/, notebooks/)
  - [x] Include clear error messages if prerequisites are missing
  - [x] Removed Docker/Qdrant host configuration (using in-memory)

- [x] Task 7: Add Validation Checks to Notebook (AC: 1, 2, 3, 4, 5)
  - [x] Add cell to verify all imports work correctly
  - [x] Add cell to check required environment variables are set
  - [x] Add cell to verify required directories exist (data/, golden_dataset/)
  - [x] Include clear error messages if prerequisites are missing
  - [x] Removed Qdrant connection validation (using in-memory)

## Dev Notes

### Previous Story Insights
[Source: Story 1.1, 1.2, 1.3 Dev Agent Records]

**From Story 1.1 (Project Setup):**
- Monorepo structure established with `data/`, `golden_dataset/`, `backend/`, `frontend/`, `notebooks/`, `docs/`
- Docker Compose configuration working with Qdrant, backend, and frontend services
- Python 3.13 with `uv` for dependency management
- Root `Makefile` created with targets for docker-compose operations

**From Story 1.2 (Ingestion Pipeline & RAG):**
- Ingestion Pipeline implemented in `backend/app/pipelines/ingestion_pipeline.py`
- Document loaders: PyMuPDFLoader (PDF), Docx2txtLoader (DOCX), TextLoader (TXT)
- Chunking: RecursiveCharacterTextSplitter (chunk_size=1000, chunk_overlap=200)
- Embeddings: OpenAI `text-embedding-3-small`
- Vector Store: Qdrant running on localhost:6333
- Collection name: "legal_documents"
- RAG Pipeline: Implemented in `backend/app/rag/pipeline.py` with Naive Retrieval
- LLM: OpenAI `gpt-4o-mini` with temperature 0.0
- Configuration: `backend/app/core/config.py` loads from environment variables

**From Story 1.3 (SGD Generation):**
- SGD script: `backend/app/scripts/generate_sgd.py`
- Uses RAGAS 0.37+ TestsetGenerator
- Output: `golden_dataset/sgd_benchmark.csv` with timestamp
- CSV columns: user_input, reference_contexts, reference, synthesizer_name
- Default test size: 3 (configurable via --test-size argument)
- Includes retry logic for rate limiting
- Comprehensive logging and progress indicators

### Jupytext Format and Cell Markers
[Source: architecture/6-implementation-details.md#65-notebooks-evaluation-jupytext]

**Jupytext Format:** `.py` scripts using `# %%` cell markers for notebook compatibility

**Cell Marker Syntax:**
- Code cell: `# %%`
- Markdown cell: `# %% [markdown]`
- Cell with title: `# %% Cell Title`

**Benefits:**
- Version control friendly (plain Python files)
- Can be executed as regular Python scripts
- Can be opened in Jupyter/JupyterLab as notebooks
- Supports both code and markdown cells

**Example Structure:**
```python
# %% [markdown]
# # Notebook Title
# Description of the notebook

# %% Setup
import pandas as pd
# ... imports

# %% Load Data
data = pd.read_csv('file.csv')
```

### Notebook Location and Purpose
[Source: architecture/2-high-level-architecture.md#23-repository-structure, architecture/6-implementation-details.md#65]

**Location:** `notebooks/E01_Pipeline_Foundation.py`

**Purpose:** 
- **Demonstration and examination** of Epic 1 implementations
- Interactive validation of ingestion, RAG, and SGD generation capabilities
- Providing executable examples for examiners to run and verify system functionality
- Serving as documentation through executable examples

**Naming Convention:** `E{epic_num}_{description}.py`

### Required Imports and Dependencies
[Source: Stories 1.2, 1.3 implementations]

**Core Dependencies:**
- `langchain` - For RAG pipeline and document processing
- `langchain_openai` - For OpenAI LLM and embeddings
- `langchain_qdrant` - For Qdrant vector store integration
- `ragas` - For SGD generation (if running generation in notebook)
- `pandas` - For loading and displaying CSV data
- `python-dotenv` - For loading environment variables
- Standard library: `os`, `pathlib`, `json`, `logging`

**Project Imports:**
- `backend.app.pipelines.ingestion_pipeline` - Ingestion Pipeline
- `backend.app.rag.pipeline` - RAG pipeline
- `backend.app.core.config` - Configuration settings
- `backend.app.scripts.generate_sgd` - SGD generation (if importing functions)

### Environment Configuration
[Source: Story 1.2 Dev Notes]

**Required Environment Variables:**
- `OPENAI_API_KEY` - For LLM and embeddings
- `QDRANT_HOST` - Qdrant connection (default: localhost)
- `QDRANT_PORT` - Qdrant port (default: 6333)

**Configuration File:** `backend/app/core/config.py`
**Environment File:** `backend/.env` (template in `.env.example`)

### Sample Documents Available
[Source: Project file structure]

**Documents in `data/` directory:**
- Agreement_PlanOfMerger_Pepco.pdf
- Asset_Purchase_Agreement_RathGibson.pdf
- Freedom_Final_Asset_Agreement.pdf
- Non_Compete_Agreement.pdf
- Stock_Purchase_Agreement.pdf

**Recommendation:** Use one of the smaller documents for testing to minimize processing time and API costs.

### RAG Pipeline Configuration
[Source: Story 1.2 implementation, architecture/6-implementation-details.md#63]

**Pipeline Components:**
- **Chunking Strategy:** RecursiveCharacterTextSplitter (default from Story 1.2)
- **Retriever:** Naive Retrieval (basic similarity search)
- **Vector Store:** Qdrant collection "legal_documents"
- **LLM:** OpenAI gpt-4o-mini (temperature 0.0)
- **Embeddings:** OpenAI text-embedding-3-small

**Pipeline Location:** `backend/app/rag/pipeline.py`

**Usage Pattern:**
```python
from backend.app.rag.pipeline import create_rag_chain

# Create RAG chain
rag_chain = create_rag_chain(
    retriever_type="naive",
    collection_name="legal_documents"
)

# Query the chain
response = rag_chain.invoke({"question": "What are the key terms?"})
```

### SGD Output Format
[Source: Story 1.3 Dev Notes]

**CSV Location:** `golden_dataset/sgd_benchmark.csv` (or timestamped version)

**Expected Columns (RAGAS 0.37+):**
- `user_input` - Generated question/query
- `reference_contexts` - List of relevant document chunks (as string representation of list)
- `reference` - Ground truth answer
- `synthesizer_name` - Query synthesizer used (e.g., SingleHopSpecificQuerySynthesizer)

**Display Recommendations:**
- Show first 3-5 rows for sample
- Display column names and data types
- Show summary statistics (total rows, unique synthesizers)
- Truncate long text fields for readability

### Testing Standards
[Source: architecture/3-tech-stack.md, Stories 1.2, 1.3]

**Testing Framework:** Pytest

**Test Location:** `backend/tests/test_notebook_e01.py`

**Test Requirements:**
1. **Import Tests:**
   - Verify notebook script can be imported without errors
   - Check all required dependencies are available
   - Validate Jupytext cell markers are correctly formatted

2. **Execution Tests (Optional):**
   - Test notebook can execute in headless mode
   - Verify outputs are generated correctly
   - Check error handling for missing prerequisites

3. **Integration Tests:**
   - Test interaction with Qdrant (if running)
   - Verify document loading works
   - Check SGD CSV can be loaded and parsed

**Mocking Strategy:**
- Mock OpenAI API calls to avoid costs during testing
- Use small sample documents for integration tests
- Mock Qdrant operations if vector store not available in test environment

### Notebook Best Practices
[Source: architecture/6-implementation-details.md#65]

**Code Organization:**
- Group related imports in setup cells
- Use clear cell titles for navigation
- Include markdown cells for explanations
- Add error handling and informative output
- Display results in readable format (tables, formatted text)

**Documentation:**
- Explain what each cell does
- Document expected outputs
- Include troubleshooting tips
- Add links to relevant documentation

**Performance:**
- Use small sample sizes for demonstrations
- Add timing information for long-running operations
- Cache results where appropriate
- Provide progress indicators

### Project Structure Notes
- The `notebooks/` directory already exists from Story 1.1
- Notebook should be executable both as a Python script and in Jupyter
- Should work within Docker environment (backend container has Jupyter installed)
- Can also be run locally if environment is set up

### Execution Context
**Running in Docker:**
```bash
docker-compose exec backend jupyter notebook notebooks/E01_Pipeline_Foundation.py
```

**Running Locally:**
```bash
cd backend
python -m jupyter notebook ../notebooks/E01_Pipeline_Foundation.py
```

**Running as Script:**
```bash
cd backend
python ../notebooks/E01_Pipeline_Foundation.py
```

## Testing

### Validation Approach
This notebook serves as an **interactive demonstration tool** for examiners, not a unit test suite. Validation is performed through:
1. In-notebook execution checks (Task 7)
2. Visual output verification by examiners
3. Sample data display and results inspection

### In-Notebook Validation (Task 7)
The notebook includes validation cells that check:
- All required imports are available
- Environment variables are properly set
- Required directories exist (data/, golden_dataset/)
- Qdrant connection is functional
- Clear error messages guide users if prerequisites are missing

### Optional: Automated Validation
If automated validation is desired for CI/CD purposes, create: `backend/tests/test_notebook_e01.py`

**Potential Automated Tests:**
1. **Import and Syntax Tests:**
   - Test notebook script can be imported without syntax errors
   - Verify all required dependencies are importable
   - Check Jupytext cell markers are correctly formatted

2. **Cell Structure Tests:**
   - Verify presence of all required cells (Docs, Ingest, RAG, SGD)
   - Check markdown cells have proper formatting
   - Validate code cells have proper structure

3. **Execution Tests (with mocking):**
   - Test notebook can execute without errors (with mocked API calls)
   - Verify outputs are generated in expected format
   - Mock OpenAI API calls to avoid costs
   - Mock Qdrant operations if vector store not available

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-20 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (claude-3-5-sonnet-20241022)

### Debug Log References
No debug log entries required - implementation was straightforward.

### Completion Notes List
- Notebook refactored to use **in-memory Qdrant** (no Docker required!)
- All acceptance criteria met:
  - ✅ AC1: Script created with Jupytext format (# %% markers)
  - ✅ AC2: Cell 1 explains purpose, prerequisites, and setup
  - ✅ AC3: Cell 2 runs ingestion pipeline on sample PDF from data/ (in-memory)
  - ✅ AC4: Cell 3 tests RAG with Naive Retrieval and displays results
  - ✅ AC5: Cell 4 runs SGD generation and displays CSV output
- Key improvements over original implementation:
  - **No Docker dependency** - uses `QdrantClient(location=":memory:")`
  - **Completely self-contained** - only needs Python dependencies
  - **Faster startup** - no container spin-up time
  - **Easier for examiners** - just install deps and run
- All cells include proper error handling and informative output
- Environment validation integrated into setup cell
- Notebook can be run as Python script or opened in Jupyter
- Uses first available PDF from data/ directory for demonstration
- Data is ephemeral (lost on restart) but acceptable for demo purposes

### File List
**Modified:**
- `notebooks/E01_Pipeline_Foundation.py` - Refactored to use in-memory Qdrant instead of Docker

**Created (optional):**
- `backend/tests/test_notebook_e01.py` - Test suite for notebook validation (per user request, tests not strictly required)

## QA Results
_To be filled by QA Agent after implementation review_
