{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97717d16",
   "metadata": {},
   "source": [
    "# E01: Pipeline Foundation - Epic 1 Validation\n",
    "\n",
    "## Purpose\n",
    "This notebook validates the implementations from Epic 1:\n",
    "- **Story 1.2**: Ingestion Agent & RAG Pipeline\n",
    "- **Story 1.3**: Synthetic Golden Dataset (SGD) Generation\n",
    "\n",
    "## What This Notebook Demonstrates\n",
    "1. **Document Ingestion**: Loading and processing legal documents into in-memory vector store\n",
    "2. **RAG Pipeline**: Querying documents using Naive Retrieval\n",
    "3. **SGD Generation**: Creating synthetic test datasets with RAGAS\n",
    "\n",
    "## Prerequisites\n",
    "- Environment variables configured (OPENAI_API_KEY)\n",
    "- Sample documents in `data/` directory\n",
    "- Backend dependencies installed (`cd backend && uv sync`)\n",
    "\n",
    "## Setup Instructions\n",
    "1. Set environment variables in `backend/.env` (or export OPENAI_API_KEY)\n",
    "2. Install dependencies: `cd backend && uv sync`\n",
    "3. Run this notebook from the project root or backend directory\n",
    "\n",
    "## Note\n",
    "This notebook uses **in-memory Qdrant** - no Docker required!\n",
    "Data is ephemeral and will be lost when the notebook restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f671fd8",
   "metadata": {
    "title": "Setup and Imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add backend to Python path if running from notebooks directory\n",
    "backend_path = Path(__file__).parent.parent / \"backend\"\n",
    "if backend_path.exists():\n",
    "    sys.path.insert(0, str(backend_path))\n",
    "\n",
    "# Load environment variables\n",
    "env_path = backend_path / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: .env file not found. Using system environment variables.\")\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "print(f\"   OPENAI_API_KEY: {'*' * 20}{os.getenv('OPENAI_API_KEY', '')[-4:]}\")\n",
    "print(f\"   Using in-memory Qdrant (no Docker required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1913a",
   "metadata": {},
   "source": [
    "## Cell 2: Document Ingestion Test\n",
    "\n",
    "This cell demonstrates the Ingestion Agent from Story 1.2:\n",
    "- Loads a sample legal document from the `data/` directory\n",
    "- Processes it using RecursiveCharacterTextSplitter (chunk_size=1000, overlap=200)\n",
    "- Stores embeddings in **in-memory Qdrant vector store**\n",
    "- Displays ingestion results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2c7a9",
   "metadata": {
    "title": "Document Ingestion"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Select a sample document\n",
    "data_dir = Path(__file__).parent.parent / \"data\"\n",
    "sample_docs = list(data_dir.glob(\"*.pdf\"))\n",
    "\n",
    "if not sample_docs:\n",
    "    raise FileNotFoundError(f\"No PDF documents found in {data_dir}\")\n",
    "\n",
    "# Use the first available document\n",
    "sample_doc = sample_docs[0]\n",
    "print(f\"üìÑ Ingesting document: {sample_doc.name}\")\n",
    "print(f\"   File size: {sample_doc.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "try:\n",
    "    # Load document\n",
    "    loader = PyMuPDFLoader(str(sample_doc))\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create in-memory Qdrant client\n",
    "    qdrant_client = QdrantClient(location=\":memory:\")\n",
    "    \n",
    "    # Create collection\n",
    "    collection_name = \"legal_documents\"\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    "    )\n",
    "    \n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    \n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(chunks)\n",
    "    \n",
    "    print(\"\\n‚úÖ Ingestion Complete!\")\n",
    "    print(f\"   Document: {sample_doc.name}\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print(f\"   Collection: {collection_name} (in-memory)\")\n",
    "    \n",
    "    # Display sample chunks\n",
    "    print(\"\\nüìù Sample Chunks (first 2):\")\n",
    "    for i, chunk in enumerate(chunks[:2], 1):\n",
    "        preview = chunk.page_content[:200] + \"...\" if len(chunk.page_content) > 200 else chunk.page_content\n",
    "        print(f\"\\n   Chunk {i}:\")\n",
    "        print(f\"   {preview}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ingestion failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84a23a",
   "metadata": {},
   "source": [
    "## Cell 3: RAG Pipeline Test\n",
    "\n",
    "This cell demonstrates the RAG pipeline from Story 1.2:\n",
    "- Uses Naive Retrieval (basic similarity search)\n",
    "- Queries the in-memory vector store\n",
    "- Displays retrieved context and generated answer\n",
    "- Shows performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c331a",
   "metadata": {
    "title": "RAG Pipeline Test"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a test question relevant to legal documents\n",
    "test_question = \"What are the key parties involved in this agreement?\"\n",
    "\n",
    "print(f\"‚ùì Question: {test_question}\\n\")\n",
    "\n",
    "try:\n",
    "    print(\"üîß Creating RAG chain with Naive Retrieval...\")\n",
    "    \n",
    "    # Create retriever from vector store\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Create LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    \n",
    "    # Create prompt template\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    \n",
    "    # Create chains\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    # Execute query and measure time\n",
    "    start_time = time.time()\n",
    "    response = rag_chain.invoke({\"input\": test_question})\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"‚úÖ RAG Query Complete!\\n\")\n",
    "    print(f\"‚è±Ô∏è  Response Time: {elapsed_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Display retrieved context\n",
    "    if 'context' in response:\n",
    "        contexts = response['context']\n",
    "        print(f\"üìö Retrieved Contexts ({len(contexts)} chunks):\")\n",
    "        for i, ctx in enumerate(contexts[:3], 1):  # Show first 3\n",
    "            preview = ctx.page_content[:200] + \"...\" if len(ctx.page_content) > 200 else ctx.page_content\n",
    "            print(f\"\\n   Context {i}:\")\n",
    "            print(f\"   {preview}\")\n",
    "    \n",
    "    # Display answer\n",
    "    answer = response.get('answer', '')\n",
    "    print(f\"\\nüí° Generated Answer:\")\n",
    "    print(f\"   {answer}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå RAG query failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888e3b2",
   "metadata": {},
   "source": [
    "## Cell 4: Synthetic Golden Dataset (SGD) Generation\n",
    "\n",
    "This cell demonstrates SGD generation from Story 1.3:\n",
    "- Runs the SGD generation script with a small test size\n",
    "- Loads the generated CSV from `golden_dataset/`\n",
    "- Displays sample questions and metadata\n",
    "- Shows summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7ace9",
   "metadata": {
    "title": "SGD Generation and Display"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run SGD generation with small test size to minimize API costs\n",
    "print(\"üî¨ Generating Synthetic Golden Dataset (test size: 3)...\")\n",
    "print(\"   This may take a few minutes due to API calls...\\n\")\n",
    "\n",
    "try:\n",
    "    # Run the SGD generation script\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"app.scripts.generate_sgd\", \"--test-size\", \"3\"],\n",
    "        cwd=str(backend_path),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=300  # 5 minute timeout\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ö†Ô∏è  SGD generation had issues:\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"‚úÖ SGD generation complete!\")\n",
    "        print(result.stdout)\n",
    "    \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚ö†Ô∏è  SGD generation timed out (>5 minutes)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  SGD generation error: {str(e)}\")\n",
    "\n",
    "# Load and display the generated SGD\n",
    "golden_dataset_dir = Path(__file__).parent.parent / \"golden_dataset\"\n",
    "csv_files = sorted(golden_dataset_dir.glob(\"sgd_benchmark*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"\\n‚ùå No SGD CSV files found in golden_dataset/\")\n",
    "else:\n",
    "    # Use the most recent file\n",
    "    latest_csv = csv_files[-1]\n",
    "    print(f\"\\nüìä Loading SGD from: {latest_csv.name}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(latest_csv)\n",
    "        \n",
    "        print(f\"\\n‚úÖ SGD Loaded Successfully!\")\n",
    "        print(f\"   Total questions: {len(df)}\")\n",
    "        print(f\"   Columns: {', '.join(df.columns.tolist())}\")\n",
    "        \n",
    "        # Show summary statistics\n",
    "        if 'synthesizer_name' in df.columns:\n",
    "            print(f\"\\nüìà Question Type Distribution:\")\n",
    "            synthesizer_counts = df['synthesizer_name'].value_counts()\n",
    "            for synthesizer, count in synthesizer_counts.items():\n",
    "                print(f\"   {synthesizer}: {count}\")\n",
    "        \n",
    "        # Display sample rows\n",
    "        print(f\"\\nüìù Sample Questions (first 3 rows):\")\n",
    "        pd.set_option('display.max_colwidth', 100)\n",
    "        pd.set_option('display.width', 120)\n",
    "        \n",
    "        for idx, row in df.head(3).iterrows():\n",
    "            print(f\"\\n   Question {idx + 1}:\")\n",
    "            print(f\"   User Input: {row['user_input'][:150]}...\")\n",
    "            \n",
    "            if 'reference' in row and pd.notna(row['reference']):\n",
    "                print(f\"   Reference: {str(row['reference'])[:150]}...\")\n",
    "            \n",
    "            if 'synthesizer_name' in row:\n",
    "                print(f\"   Synthesizer: {row['synthesizer_name']}\")\n",
    "        \n",
    "        # Display full dataframe info\n",
    "        print(f\"\\nüìã Full Dataset Info:\")\n",
    "        print(df.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading SGD CSV: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe33764",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has validated the Epic 1 implementations:\n",
    "\n",
    "‚úÖ **Document Ingestion**: Successfully loaded and chunked legal documents into Qdrant\n",
    "‚úÖ **RAG Pipeline**: Retrieved relevant context and generated answers using Naive Retrieval\n",
    "‚úÖ **SGD Generation**: Created synthetic test dataset with RAGAS\n",
    "\n",
    "### Next Steps\n",
    "- Epic 2: Advanced RAG techniques (BM25, Multi-Query, Reranking)\n",
    "- Epic 3: Agent implementation (Clause Extraction, Risk Scoring, etc.)\n",
    "- Epic 4: Agent orchestration with LangGraph\n",
    "\n",
    "### Troubleshooting\n",
    "- **OpenAI API Error**: Check OPENAI_API_KEY in `.env` file or environment variables\n",
    "- **Import Errors**: Ensure backend dependencies are installed (`cd backend && uv sync`)\n",
    "- **Memory Issues**: If processing large documents, consider reducing chunk size or using fewer documents"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
