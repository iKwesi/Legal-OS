{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ad121e",
   "metadata": {},
   "source": [
    "# Legal-OS: Complete M&A Document Analysis System\n",
    "\n",
    "This notebook demonstrates the entire Legal-OS pipeline in a single, cohesive workflow.\n",
    "It combines all Epic demonstrations (E01-E08) with shared resources and progressive complexity.\n",
    "\n",
    "## System Overview\n",
    "\n",
    "Legal-OS is an AI-powered M&A due diligence system featuring:\n",
    "- **Document Ingestion & RAG** - Process legal documents with vector search\n",
    "- **Clause Extraction** - Identify M&A clauses with LangGraph agents\n",
    "- **Risk Scoring** - Assess risks with rule-based heuristics\n",
    "- **Summary Generation** - Create comprehensive diligence memos\n",
    "- **Source Tracking** - Maintain provenance for all findings\n",
    "- **Checklist Generation** - Produce actionable follow-up items\n",
    "- **Orchestration** - Coordinate all agents in a unified workflow\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Setup & Configuration** - One-time environment setup\n",
    "2. **Document Ingestion** - Load and process M&A document (shared across all agents)\n",
    "3. **RAG Pipeline** - Test retrieval and question answering\n",
    "4. **Agent Demonstrations** - Sequential agent workflow\n",
    "5. **End-to-End Orchestration** - Complete automated pipeline\n",
    "6. **Results & Export** - Consolidated results and exports\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Environment variables configured (OPENAI_API_KEY)\n",
    "- Sample M&A documents in `data/` directory\n",
    "- Backend dependencies installed (`cd backend && uv sync`)\n",
    "\n",
    "## Note\n",
    "\n",
    "This notebook uses **in-memory Qdrant** - no Docker required!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11c456",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup & Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a16c81",
   "metadata": {
    "title": "Setup and Imports"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LEGAL-OS: M&A DOCUMENT ANALYSIS SYSTEM\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Environment setup complete\n",
      "   OPENAI_API_KEY: ********************P2EA\n",
      "   Using in-memory Qdrant (no Docker required)\n",
      "   Backend path: /Users/rbblankson34/Documents/Projects/Legal_AI/Legal-OS/backend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "from collections import defaultdict\n",
    "\n",
    "# Try to import IPython for visualization\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    IPYTHON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IPYTHON_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  IPython not available - visualization will be skipped\")\n",
    "\n",
    "# Add backend to Python path\n",
    "# Handle both script and notebook execution\n",
    "try:\n",
    "    backend_path = Path(__file__).parent.parent / \"backend\"\n",
    "except NameError:\n",
    "    # __file__ not defined in Jupyter notebooks\n",
    "    backend_path = Path.cwd() / \"backend\"\n",
    "    if not backend_path.exists():\n",
    "        backend_path = Path.cwd().parent / \"backend\"\n",
    "\n",
    "if backend_path.exists():\n",
    "    sys.path.insert(0, str(backend_path))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Backend path not found. Tried: {backend_path}\")\n",
    "\n",
    "# Load environment variables\n",
    "env_path = backend_path / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LEGAL-OS: M&A DOCUMENT ANALYSIS SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Environment setup complete\")\n",
    "print(f\"   OPENAI_API_KEY: {'*' * 20}{os.getenv('OPENAI_API_KEY', '')[-4:]}\")\n",
    "print(f\"   Using in-memory Qdrant (no Docker required)\")\n",
    "print(f\"   Backend path: {backend_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce78cd",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Document Ingestion (Shared Resource)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b3068f",
   "metadata": {
    "title": "Document Ingestion"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENT INGESTION\n",
      "================================================================================\n",
      "\n",
      "üìÑ Document: Freedom_Final_Asset_Agreement.pdf\n",
      "   Size: 284.84 KB\n",
      "\n",
      "‚úÖ Ingestion Complete! (6.42s)\n",
      "   Document ID: 49f281c5-45aa-414d-9598-6fcd51189e5e\n",
      "   Total chunks: 345\n",
      "   This vector store will be reused across all agents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.pipelines.ingestion_pipeline import IngestionPipeline\n",
    "\n",
    "# Handle both script and notebook execution\n",
    "try:\n",
    "    data_dir = Path(__file__).parent.parent / \"data\"\n",
    "except NameError:\n",
    "    # __file__ not defined in Jupyter notebooks\n",
    "    data_dir = Path.cwd() / \"data\"\n",
    "    if not data_dir.exists():\n",
    "        data_dir = Path.cwd().parent / \"data\"\n",
    "        \n",
    "freedom_doc = data_dir / \"Freedom_Final_Asset_Agreement.pdf\"\n",
    "\n",
    "if not freedom_doc.exists():\n",
    "    sample_docs = list(data_dir.glob(\"*.pdf\"))\n",
    "    if not sample_docs:\n",
    "        raise FileNotFoundError(f\"No PDF documents found in {data_dir}\")\n",
    "    sample_doc = sample_docs[0]\n",
    "    print(f\"‚ö†Ô∏è  Freedom document not found, using {sample_doc.name}\")\n",
    "else:\n",
    "    sample_doc = freedom_doc\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DOCUMENT INGESTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÑ Document: {sample_doc.name}\")\n",
    "print(f\"   Size: {sample_doc.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Create ingestion pipeline with in-memory vector store\n",
    "pipeline = IngestionPipeline(use_memory=True)\n",
    "session_id = str(uuid4())\n",
    "\n",
    "# Ingest document\n",
    "ingestion_start = time.time()\n",
    "result = pipeline.ingest_document(file_path=str(sample_doc), session_id=session_id)\n",
    "ingestion_time = time.time() - ingestion_start\n",
    "\n",
    "# Get shared resources\n",
    "vector_store = pipeline.vector_store\n",
    "document_id = result[\"document_id\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Ingestion Complete! ({ingestion_time:.2f}s)\")\n",
    "print(f\"   Document ID: {document_id}\")\n",
    "print(f\"   Total chunks: {result['chunk_count']}\")\n",
    "print(f\"   This vector store will be reused across all agents.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db616a1",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: RAG Pipeline Test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de46cb5",
   "metadata": {
    "title": "RAG Pipeline Test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAG PIPELINE TEST\n",
      "================================================================================\n",
      "\n",
      "‚ùì Question: What are the key parties involved in this agreement?\n",
      "\n",
      "‚úÖ RAG Query Complete! (1.46s)\n",
      "\n",
      "üí° Answer:\n",
      "   The key parties involved in this agreement are the Buyer and the Seller. Additionally, their respective successors and permitted assigns are also considered parties to the agreement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RAG PIPELINE TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_question = \"What are the key parties involved in this agreement?\"\n",
    "print(f\"\\n‚ùì Question: {test_question}\")\n",
    "\n",
    "# Create RAG chain\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Execute query\n",
    "rag_start = time.time()\n",
    "response = rag_chain.invoke({\"input\": test_question})\n",
    "rag_time = time.time() - rag_start\n",
    "\n",
    "print(f\"\\n‚úÖ RAG Query Complete! ({rag_time:.2f}s)\")\n",
    "print(f\"\\nüí° Answer:\")\n",
    "print(f\"   {response.get('answer', '')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81cecf",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3.5: LangSmith Setup (Optional)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe7bbcb",
   "metadata": {
    "title": "LangSmith Setup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LANGSMITH TRACING SETUP\n",
      "================================================================================\n",
      "\n",
      "‚úÖ LangSmith tracing enabled\n",
      "   API Key: ********************1de5\n",
      "   Project: Legal-OS-Evaluation\n",
      "   View traces at: https://smith.langchain.com/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LANGSMITH TRACING SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for LangSmith API key\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "if langsmith_api_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"Legal-OS-Evaluation\"\n",
    "    print(\"\\n‚úÖ LangSmith tracing enabled\")\n",
    "    print(f\"   API Key: {'*' * 20}{langsmith_api_key[-4:]}\")\n",
    "    print(f\"   Project: Legal-OS-Evaluation\")\n",
    "    print(f\"   View traces at: https://smith.langchain.com/\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  LangSmith tracing disabled\")\n",
    "    print(\"   To enable: Add LANGCHAIN_API_KEY to backend/.env\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5644ccc",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3.6: Comprehensive RAG Evaluation with RAGAS\n",
    "---\n",
    "\n",
    "This section evaluates 10 different retriever configurations:\n",
    "- 2 chunking strategies (Naive, Semantic)\n",
    "- 5 retrieval methods (Vector, BM25, Multi-Query, Ensemble, Cohere Rerank)\n",
    "\n",
    "**Note:** This evaluation takes 15-20 minutes to complete.\n",
    "Set RUN_EVALUATION = False to skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a741afd2",
   "metadata": {
    "title": "Evaluation Configuration"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAG EVALUATION WITH RAGAS\n",
      "================================================================================\n",
      "\n",
      "üìä Evaluation Configuration:\n",
      "   Run Evaluation: True\n",
      "   Cohere API Key: ‚úÖ Configured\n",
      "   Expected configurations: 10\n",
      "   Estimated time: 15 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RAG EVALUATION WITH RAGAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Toggle to enable/disable evaluation\n",
    "RUN_EVALUATION = True  # Set to False to skip evaluation\n",
    "\n",
    "# Check for Cohere API key\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "has_cohere = cohere_api_key is not None\n",
    "\n",
    "print(f\"\\nüìä Evaluation Configuration:\")\n",
    "print(f\"   Run Evaluation: {RUN_EVALUATION}\")\n",
    "print(f\"   Cohere API Key: {'‚úÖ Configured' if has_cohere else '‚ùå Not configured (will skip reranking)'}\")\n",
    "print(f\"   Expected configurations: {10 if has_cohere else 8}\")\n",
    "print(f\"   Estimated time: {15 if has_cohere else 12} minutes\\n\")\n",
    "\n",
    "if not RUN_EVALUATION:\n",
    "    print(\"‚è≠Ô∏è  Skipping RAG evaluation (RUN_EVALUATION=False)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa9e53",
   "metadata": {
    "title": "Run Evaluations"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rbblankson34/Documents/Projects/Legal_AI/Legal-OS/backend/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing evaluator...\n",
      "‚úÖ Evaluator initialized\n",
      "\n",
      "üöÄ Running 10 configurations...\n",
      "\n",
      "[1/10] Evaluating: Naive + Vector Similarity\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|‚ñà‚ñà        | 8/40 [00:07<00:21,  1.49it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:13<00:12,  1.89it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:45<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 77.96s\n",
      "   Metrics: Precision=0.6235, Recall=0.1600, Faithfulness=0.8833, Relevancy=0.2892\n",
      "\n",
      "[2/10] Evaluating: Naive + BM25 Keyword\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|‚ñà‚ñå        | 6/40 [00:07<00:35,  1.05s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñé       | 9/40 [00:10<00:29,  1.05it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:48<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 71.13s\n",
      "   Metrics: Precision=0.5671, Recall=0.2100, Faithfulness=0.8328, Relevancy=0.1839\n",
      "\n",
      "[3/10] Evaluating: Naive + Multi-Query\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|‚ñà‚ñà        | 8/40 [00:11<00:29,  1.10it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:19<00:22,  1.09it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 22/40 [00:27<00:22,  1.27s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 120.76s\n",
      "   Metrics: Precision=0.6089, Recall=0.1433, Faithfulness=0.7679, Relevancy=0.2852\n",
      "\n",
      "[4/10] Evaluating: Naive + Ensemble (BM25+Vector)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|‚ñà‚ñå        | 6/40 [00:07<00:32,  1.03it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:13<00:13,  1.72it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 17/40 [00:16<00:19,  1.20it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 21/40 [00:20<00:17,  1.09it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 26/40 [00:30<00:22,  1.61s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:00<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 95.79s\n",
      "   Metrics: Precision=0.6227, Recall=0.2483, Faithfulness=0.8854, Relevancy=0.3730\n",
      "\n",
      "[5/10] Evaluating: Naive + Cohere Reranking\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 10/40 [00:09<00:22,  1.34it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 17/40 [00:14<00:14,  1.63it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 18/40 [00:17<00:21,  1.04it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:44<00:00,  1.12s/it]\n",
      "SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 84.57s\n",
      "   Metrics: Precision=0.6579, Recall=0.1983, Faithfulness=0.7869, Relevancy=0.3789\n",
      "\n",
      "[6/10] Evaluating: Semantic + Vector Similarity\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:15<00:14,  1.66it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 19/40 [00:21<00:25,  1.23s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:46<00:00,  1.17s/it]\n",
      "SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 78.90s\n",
      "   Metrics: Precision=0.6015, Recall=0.1767, Faithfulness=0.8026, Relevancy=0.2861\n",
      "\n",
      "[7/10] Evaluating: Semantic + BM25 Keyword\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|‚ñà‚ñà‚ñé       | 9/40 [00:08<00:19,  1.56it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 10/40 [00:10<00:24,  1.20it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 17/40 [00:14<00:13,  1.76it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 22/40 [00:20<00:15,  1.14it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 26/40 [00:26<00:16,  1.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:45<00:00,  1.13s/it]\n",
      "SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 64.65s\n",
      "   Metrics: Precision=0.5288, Recall=0.1150, Faithfulness=0.8935, Relevancy=0.1839\n",
      "\n",
      "[8/10] Evaluating: Semantic + Multi-Query\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|‚ñà‚ñà        | 8/40 [00:07<00:21,  1.50it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:13<00:12,  1.99it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 24/40 [00:29<00:38,  2.41s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:57<00:00,  1.44s/it]\n",
      "SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 113.74s\n",
      "   Metrics: Precision=0.6410, Recall=0.2183, Faithfulness=0.8988, Relevancy=0.2852\n",
      "\n",
      "[9/10] Evaluating: Semantic + Ensemble (BM25+Vector)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 24/40 [00:27<00:28,  1.80s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:08<00:00,  1.72s/it]\n",
      "SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 105.57s\n",
      "   Metrics: Precision=0.5315, Recall=0.4683, Faithfulness=0.8471, Relevancy=0.3774\n",
      "\n",
      "[10/10] Evaluating: Semantic + Cohere Reranking\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|‚ñà‚ñà        | 8/40 [00:08<00:24,  1.32it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 18/40 [00:17<00:17,  1.23it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 21/40 [00:21<00:19,  1.01s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 24/40 [00:30<00:34,  2.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:47<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete in 83.36s\n",
      "   Metrics: Precision=0.5734, Recall=0.2150, Faithfulness=0.8783, Relevancy=0.3791\n",
      "\n",
      "‚úÖ All 10 evaluations complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVALUATION:\n",
    "    from app.rag.evaluation_langchain import LangChainRAGEvaluator\n",
    "    from app.core.config import settings\n",
    "    \n",
    "    print(\"üîß Initializing evaluator...\")\n",
    "    evaluator = LangChainRAGEvaluator(sgd_path=\"/Users/rbblankson34/Documents/Projects/Legal_AI/Legal-OS/backend/golden_dataset/sgd_benchmark.csv\")\n",
    "    print(\"‚úÖ Evaluator initialized\\n\")\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Configuration matrix\n",
    "    chunking_strategies = [\n",
    "        (\"naive\", {\"chunk_size\": settings.chunk_size, \"chunk_overlap\": settings.chunk_overlap}),\n",
    "        (\"semantic\", {\"breakpoint_threshold_type\": \"percentile\", \"breakpoint_threshold_amount\": 95.0}),\n",
    "    ]\n",
    "    \n",
    "    retriever_configs = [\n",
    "        (\"Vector Similarity\", \"evaluate_naive_retrieval\", {}),\n",
    "        (\"BM25 Keyword\", \"evaluate_bm25_retrieval\", {}),\n",
    "        (\"Multi-Query\", \"evaluate_multiquery_retrieval\", {}),\n",
    "        (\"Ensemble (Vector+BM25+MultiQuery)\", \"evaluate_ensemble_retrieval\", {}),\n",
    "    ]\n",
    "    \n",
    "    # Add Cohere reranking if API key available\n",
    "    if has_cohere:\n",
    "        retriever_configs.append(\n",
    "            (\"Cohere Reranking\", \"evaluate_contextual_compression_retrieval\", {\"cohere_api_key\": cohere_api_key})\n",
    "        )\n",
    "    \n",
    "    total_configs = len(chunking_strategies) * len(retriever_configs)\n",
    "    current_config = 0\n",
    "    \n",
    "    print(f\"üöÄ Running {total_configs} configurations...\\n\")\n",
    "    \n",
    "    # Run all combinations\n",
    "    for chunking_name, chunking_params in chunking_strategies:\n",
    "        for retriever_name, method_name, extra_params in retriever_configs:\n",
    "            current_config += 1\n",
    "            config_name = f\"{chunking_name.title()} + {retriever_name}\"\n",
    "            \n",
    "            print(f\"[{current_config}/{total_configs}] Evaluating: {config_name}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            try:\n",
    "                # Get the evaluation method\n",
    "                eval_method = getattr(evaluator, method_name)\n",
    "                \n",
    "                # Run evaluation\n",
    "                result = eval_method(\n",
    "                    chunking_strategy=chunking_name,\n",
    "                    chunking_params=chunking_params,\n",
    "                    top_k=10,\n",
    "                    **extra_params\n",
    "                )\n",
    "                \n",
    "                # Add configuration name\n",
    "                result[\"configuration\"] = config_name\n",
    "                all_results.append(result)\n",
    "                \n",
    "                print(f\"‚úÖ Complete in {result['execution_time_seconds']:.2f}s\")\n",
    "                print(f\"   Metrics: Precision={result['metrics']['context_precision']:.4f}, \"\n",
    "                      f\"Recall={result['metrics']['context_recall']:.4f}, \"\n",
    "                      f\"Faithfulness={result['metrics']['faithfulness']:.4f}, \"\n",
    "                      f\"Relevancy={result['metrics']['answer_relevancy']:.4f}\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {str(e)}\")\n",
    "                print()\n",
    "    \n",
    "    print(f\"‚úÖ All {len(all_results)} evaluations complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3ca5b",
   "metadata": {},
   "source": [
    "## Evaluation Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc118d",
   "metadata": {
    "title": "Results Comparison"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "COMPREHENSIVE RAGAS EVALUATION RESULTS\n",
      "========================================================================================================================\n",
      "\n",
      "                    Configuration Chunking                     Retriever  k  Precision   Recall  Faithfulness  Relevancy   Time (s)\n",
      "        Naive + Vector Similarity    naive                         naive 10   0.623494 0.160000      0.883333   0.289219  77.957783\n",
      "             Naive + BM25 Keyword    naive                          bm25 10   0.567144 0.210000      0.832843   0.183868  71.129481\n",
      "              Naive + Multi-Query    naive                    multiquery 10   0.608922 0.143333      0.767857   0.285229 120.762892\n",
      "   Naive + Ensemble (BM25+Vector)    naive                      ensemble 10   0.622735 0.248333      0.885354   0.373040  95.791787\n",
      "         Naive + Cohere Reranking    naive contextual_compression_cohere 10   0.657876 0.198333      0.786905   0.378891  84.572910\n",
      "     Semantic + Vector Similarity semantic                         naive 10   0.601534 0.176667      0.802632   0.286115  78.895694\n",
      "          Semantic + BM25 Keyword semantic                          bm25 10   0.528781 0.115000      0.893462   0.183868  64.647439\n",
      "           Semantic + Multi-Query semantic                    multiquery 10   0.641039 0.218333      0.898804   0.285229 113.736395\n",
      "Semantic + Ensemble (BM25+Vector) semantic                      ensemble 10   0.531460 0.468333      0.847101   0.377438 105.574941\n",
      "      Semantic + Cohere Reranking semantic contextual_compression_cohere 10   0.573396 0.215000      0.878333   0.379061  83.363883\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "DETAILED RESULTS BY CONFIGURATION\n",
      "========================================================================================================================\n",
      "\n",
      "[1] Naive + Vector Similarity\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Naive\n",
      "   Retriever: naive\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.6235\n",
      "      Context Recall:     0.1600\n",
      "      Faithfulness:       0.8833\n",
      "      Answer Relevancy:   0.2892\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 77.96s\n",
      "\n",
      "[2] Naive + BM25 Keyword\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Naive\n",
      "   Retriever: bm25\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.5671\n",
      "      Context Recall:     0.2100\n",
      "      Faithfulness:       0.8328\n",
      "      Answer Relevancy:   0.1839\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 71.13s\n",
      "\n",
      "[3] Naive + Multi-Query\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Naive\n",
      "   Retriever: multiquery\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.6089\n",
      "      Context Recall:     0.1433\n",
      "      Faithfulness:       0.7679\n",
      "      Answer Relevancy:   0.2852\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 120.76s\n",
      "\n",
      "[4] Naive + Ensemble (BM25+Vector)\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Naive\n",
      "   Retriever: ensemble\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.6227\n",
      "      Context Recall:     0.2483\n",
      "      Faithfulness:       0.8854\n",
      "      Answer Relevancy:   0.3730\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 95.79s\n",
      "\n",
      "[5] Naive + Cohere Reranking\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Naive\n",
      "   Retriever: contextual_compression_cohere\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.6579\n",
      "      Context Recall:     0.1983\n",
      "      Faithfulness:       0.7869\n",
      "      Answer Relevancy:   0.3789\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 84.57s\n",
      "\n",
      "[6] Semantic + Vector Similarity\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Semantic\n",
      "   Retriever: naive\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.6015\n",
      "      Context Recall:     0.1767\n",
      "      Faithfulness:       0.8026\n",
      "      Answer Relevancy:   0.2861\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 78.90s\n",
      "\n",
      "[7] Semantic + BM25 Keyword\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Semantic\n",
      "   Retriever: bm25\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.5288\n",
      "      Context Recall:     0.1150\n",
      "      Faithfulness:       0.8935\n",
      "      Answer Relevancy:   0.1839\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 64.65s\n",
      "\n",
      "[8] Semantic + Multi-Query\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Semantic\n",
      "   Retriever: multiquery\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.6410\n",
      "      Context Recall:     0.2183\n",
      "      Faithfulness:       0.8988\n",
      "      Answer Relevancy:   0.2852\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 113.74s\n",
      "\n",
      "[9] Semantic + Ensemble (BM25+Vector)\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Semantic\n",
      "   Retriever: ensemble\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.5315\n",
      "      Context Recall:     0.4683\n",
      "      Faithfulness:       0.8471\n",
      "      Answer Relevancy:   0.3774\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 105.57s\n",
      "\n",
      "[10] Semantic + Cohere Reranking\n",
      "--------------------------------------------------------------------------------\n",
      "   Chunking: Semantic\n",
      "   Retriever: contextual_compression_cohere\n",
      "   Top-K: 10\n",
      "   Documents: 345\n",
      "   Samples: 10\n",
      "\n",
      "   üìä RAGAS Metrics:\n",
      "      Context Precision:  0.5734\n",
      "      Context Recall:     0.2150\n",
      "      Faithfulness:       0.8783\n",
      "      Answer Relevancy:   0.3791\n",
      "\n",
      "   ‚è±Ô∏è  Execution Time: 83.36s\n",
      "\n",
      "========================================================================================================================\n",
      "RANKINGS BY METRIC\n",
      "========================================================================================================================\n",
      "\n",
      "üèÜ Best Precision:\n",
      "   5. Naive + Cohere Reranking                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.6579\n",
      "   8. Semantic + Multi-Query                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.6410\n",
      "   1. Naive + Vector Similarity                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.6235\n",
      "   4. Naive + Ensemble (BM25+Vector)                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.6227\n",
      "   3. Naive + Multi-Query                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.6089\n",
      "\n",
      "üèÜ Best Recall:\n",
      "   9. Semantic + Ensemble (BM25+Vector)             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.4683\n",
      "   4. Naive + Ensemble (BM25+Vector)                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.2483\n",
      "   8. Semantic + Multi-Query                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.2183\n",
      "   10. Semantic + Cohere Reranking                   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.2150\n",
      "   2. Naive + BM25 Keyword                          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.2100\n",
      "\n",
      "üèÜ Best Faithfulness:\n",
      "   8. Semantic + Multi-Query                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.8988\n",
      "   7. Semantic + BM25 Keyword                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.8935\n",
      "   4. Naive + Ensemble (BM25+Vector)                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.8854\n",
      "   1. Naive + Vector Similarity                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.8833\n",
      "   10. Semantic + Cohere Reranking                   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.8783\n",
      "\n",
      "üèÜ Best Relevancy:\n",
      "   10. Semantic + Cohere Reranking                   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.3791\n",
      "   5. Naive + Cohere Reranking                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.3789\n",
      "   9. Semantic + Ensemble (BM25+Vector)             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.3774\n",
      "   4. Naive + Ensemble (BM25+Vector)                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.3730\n",
      "   1. Naive + Vector Similarity                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.2892\n",
      "\n",
      "========================================================================================================================\n",
      "üèÜ BEST OVERALL RETRIEVER (BY AVERAGE SCORE)\n",
      "========================================================================================================================\n",
      "\n",
      "Configuration: Semantic + Ensemble (BM25+Vector)\n",
      "   Context Precision:  0.5315\n",
      "   Context Recall:     0.4683\n",
      "   Faithfulness:       0.8471\n",
      "   Answer Relevancy:   0.3774\n",
      "   Average Score:      0.5561\n",
      "   Execution Time:     105.57s\n",
      "\n",
      "‚ö° FASTEST RETRIEVER\n",
      "   Configuration: Semantic + BM25 Keyword\n",
      "   Time: 64.65s\n",
      "   Average Score: 0.4303\n",
      "\n",
      "========================================================================================================================\n",
      "PERFORMANCE VS ACCURACY ANALYSIS\n",
      "========================================================================================================================\n",
      "\n",
      "Top 3 by Accuracy:\n",
      "   9. Semantic + Ensemble (BM25+Vector)             Avg=0.5561, Time=105.57s\n",
      "   4. Naive + Ensemble (BM25+Vector)                Avg=0.5324, Time= 95.79s\n",
      "   10. Semantic + Cohere Reranking                   Avg=0.5114, Time= 83.36s\n",
      "\n",
      "Top 3 by Speed:\n",
      "   7. Semantic + BM25 Keyword                       Time= 64.65s, Avg=0.4303\n",
      "   2. Naive + BM25 Keyword                          Time= 71.13s, Avg=0.4485\n",
      "   1. Naive + Vector Similarity                     Time= 77.96s, Avg=0.4890\n",
      "\n",
      "========================================================================================================================\n",
      "üí° RECOMMENDATION\n",
      "========================================================================================================================\n",
      "\n",
      "üéØ Recommended Retriever: Semantic + Ensemble (BM25+Vector)\n",
      "\n",
      "   Why this retriever:\n",
      "   ‚úÖ Highest average score (0.5561)\n",
      "   ‚úÖ Best context precision (0.5315)\n",
      "   ‚úÖ Best faithfulness (0.8471)\n",
      "\n",
      "   ‚ö†Ô∏è  Note: This is not the fastest retriever\n",
      "   ‚ö° Fastest option: Semantic + BM25 Keyword (64.65s)\n",
      "      But with lower accuracy (Avg=0.4303)\n",
      "\n",
      "   üéØ Best Balance (Score/Speed): Semantic + BM25 Keyword\n",
      "      Score: 0.4303, Time: 64.65s\n",
      "\n",
      "üíæ Evaluation results saved:\n",
      "   - ragas_evaluation_results.json (JSON - detailed results)\n",
      "   - ragas_evaluation_results.csv (CSV - comparison table)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVALUATION and all_results:\n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Configuration\": r[\"configuration\"],\n",
    "            \"Chunking\": r[\"chunking_strategy\"],\n",
    "            \"Retriever\": r[\"retriever_type\"],\n",
    "            \"k\": r[\"top_k\"],\n",
    "            \"Precision\": r[\"metrics\"][\"context_precision\"],\n",
    "            \"Recall\": r[\"metrics\"][\"context_recall\"],\n",
    "            \"Faithfulness\": r[\"metrics\"][\"faithfulness\"],\n",
    "            \"Relevancy\": r[\"metrics\"][\"answer_relevancy\"],\n",
    "            \"Time (s)\": r[\"execution_time_seconds\"],\n",
    "        }\n",
    "        for r in all_results\n",
    "    ])\n",
    "    \n",
    "    print(\"=\" * 120)\n",
    "    print(\"COMPREHENSIVE RAGAS EVALUATION RESULTS\")\n",
    "    print(\"=\" * 120)\n",
    "    print()\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Detailed results for each configuration\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"DETAILED RESULTS BY CONFIGURATION\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for i, result in enumerate(all_results, 1):\n",
    "        print(f\"\\n[{i}] {result['configuration']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"   Chunking: {result['chunking_strategy'].title()}\")\n",
    "        print(f\"   Retriever: {result['retriever_type']}\")\n",
    "        print(f\"   Top-K: {result['top_k']}\")\n",
    "        print(f\"   Documents: {result['num_documents']}\")\n",
    "        print(f\"   Samples: {result['num_samples']}\")\n",
    "        print(f\"\\n   üìä RAGAS Metrics:\")\n",
    "        print(f\"      Context Precision:  {result['metrics']['context_precision']:.4f}\")\n",
    "        print(f\"      Context Recall:     {result['metrics']['context_recall']:.4f}\")\n",
    "        print(f\"      Faithfulness:       {result['metrics']['faithfulness']:.4f}\")\n",
    "        print(f\"      Answer Relevancy:   {result['metrics']['answer_relevancy']:.4f}\")\n",
    "        print(f\"\\n   ‚è±Ô∏è  Execution Time: {result['execution_time_seconds']:.2f}s\")\n",
    "    \n",
    "    # Rankings by metric\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"RANKINGS BY METRIC\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for metric in [\"Precision\", \"Recall\", \"Faithfulness\", \"Relevancy\"]:\n",
    "        ranked = comparison_df.sort_values(metric, ascending=False)\n",
    "        print(f\"\\nüèÜ Best {metric}:\")\n",
    "        for idx, row in ranked.head(5).iterrows():\n",
    "            bar_length = int(row[metric] * 50)  # Scale to 50 chars\n",
    "            bar = \"‚ñà\" * bar_length + \"‚ñë\" * (50 - bar_length)\n",
    "            print(f\"   {idx + 1}. {row['Configuration']:45s} {bar} {row[metric]:.4f}\")\n",
    "    \n",
    "    # Overall best (by average of all metrics)\n",
    "    comparison_df[\"Average\"] = comparison_df[[\"Precision\", \"Recall\", \"Faithfulness\", \"Relevancy\"]].mean(axis=1)\n",
    "    best_overall = comparison_df.loc[comparison_df[\"Average\"].idxmax()]\n",
    "    fastest = comparison_df.loc[comparison_df[\"Time (s)\"].idxmin()]\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 120)\n",
    "    print(\"üèÜ BEST OVERALL RETRIEVER (BY AVERAGE SCORE)\")\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"\\nConfiguration: {best_overall['Configuration']}\")\n",
    "    print(f\"   Context Precision:  {best_overall['Precision']:.4f}\")\n",
    "    print(f\"   Context Recall:     {best_overall['Recall']:.4f}\")\n",
    "    print(f\"   Faithfulness:       {best_overall['Faithfulness']:.4f}\")\n",
    "    print(f\"   Answer Relevancy:   {best_overall['Relevancy']:.4f}\")\n",
    "    print(f\"   Average Score:      {best_overall['Average']:.4f}\")\n",
    "    print(f\"   Execution Time:     {best_overall['Time (s)']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n‚ö° FASTEST RETRIEVER\")\n",
    "    print(f\"   Configuration: {fastest['Configuration']}\")\n",
    "    print(f\"   Time: {fastest['Time (s)']:.2f}s\")\n",
    "    print(f\"   Average Score: {fastest['Average']:.4f}\")\n",
    "    \n",
    "    # Performance vs Accuracy Analysis\n",
    "    print(f\"\\n\" + \"=\" * 120)\n",
    "    print(\"PERFORMANCE VS ACCURACY ANALYSIS\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # Sort by average score\n",
    "    sorted_by_score = comparison_df.sort_values(\"Average\", ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 3 by Accuracy:\")\n",
    "    for idx, row in sorted_by_score.head(3).iterrows():\n",
    "        print(f\"   {idx + 1}. {row['Configuration']:45s} Avg={row['Average']:.4f}, Time={row['Time (s)']:6.2f}s\")\n",
    "    \n",
    "    print(f\"\\nTop 3 by Speed:\")\n",
    "    sorted_by_speed = comparison_df.sort_values(\"Time (s)\")\n",
    "    for idx, row in sorted_by_speed.head(3).iterrows():\n",
    "        print(f\"   {idx + 1}. {row['Configuration']:45s} Time={row['Time (s)']:6.2f}s, Avg={row['Average']:.4f}\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(f\"\\n\" + \"=\" * 120)\n",
    "    print(\"üí° RECOMMENDATION\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # Find best balance (high score, reasonable time)\n",
    "    comparison_df[\"Score_per_Second\"] = comparison_df[\"Average\"] / comparison_df[\"Time (s)\"]\n",
    "    best_balance = comparison_df.loc[comparison_df[\"Score_per_Second\"].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüéØ Recommended Retriever: {best_overall['Configuration']}\")\n",
    "    print(f\"\\n   Why this retriever:\")\n",
    "    print(f\"   ‚úÖ Highest average score ({best_overall['Average']:.4f})\")\n",
    "    print(f\"   ‚úÖ Best context precision ({best_overall['Precision']:.4f})\")\n",
    "    print(f\"   ‚úÖ Best faithfulness ({best_overall['Faithfulness']:.4f})\")\n",
    "    \n",
    "    if best_overall['Configuration'] != fastest['Configuration']:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Note: This is not the fastest retriever\")\n",
    "        print(f\"   ‚ö° Fastest option: {fastest['Configuration']} ({fastest['Time (s)']:.2f}s)\")\n",
    "        print(f\"      But with lower accuracy (Avg={fastest['Average']:.4f})\")\n",
    "    \n",
    "    print(f\"\\n   üéØ Best Balance (Score/Speed): {best_balance['Configuration']}\")\n",
    "    print(f\"      Score: {best_balance['Average']:.4f}, Time: {best_balance['Time (s)']:.2f}s\")\n",
    "    \n",
    "    # Export evaluation results\n",
    "    eval_results_path = backend_path / \"ragas_evaluation_results.json\"\n",
    "    with open(eval_results_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    \n",
    "    eval_csv_path = backend_path / \"ragas_evaluation_results.csv\"\n",
    "    comparison_df.to_csv(eval_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Evaluation results saved:\")\n",
    "    print(f\"   - {eval_results_path.name} (JSON - detailed results)\")\n",
    "    print(f\"   - {eval_csv_path.name} (CSV - comparison table)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc285a0",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Agent Demonstrations (Sequential Workflow)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff00dc",
   "metadata": {},
   "source": [
    "## 4.1: Clause Extraction Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4c75f",
   "metadata": {
    "title": "Clause Extraction Agent"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLAUSE EXTRACTION AGENT\n",
      "================================================================================\n",
      "\n",
      "ü§ñ Agent initialized (Model: gpt-4o-mini, Tools: 3)\n",
      "\n",
      "üîç Extracting clauses...\n",
      "‚úÖ Found clause: Payment Terms\n",
      "‚úÖ Found clause: Indemnification\n",
      "‚úÖ Found clause: Warranties\n",
      "‚úÖ Found red flag: High\n",
      "‚úÖ Found red flag: Medium\n",
      "‚úÖ Found red flag: Critical\n",
      "\n",
      "‚úÖ Complete! (27.92s)\n",
      "   Clauses: 3, Red Flags: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.agents.clause_extraction import ClauseExtractionAgent\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLAUSE EXTRACTION AGENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clause_agent = ClauseExtractionAgent(\n",
    "    vector_store=vector_store,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(f\"\\nü§ñ Agent initialized (Model: gpt-4o-mini, Tools: {len(clause_agent.tools)})\")\n",
    "\n",
    "# Visualize agent graph\n",
    "try:\n",
    "    graph_viz = clause_agent.get_graph_visualization()\n",
    "    if graph_viz and IPYTHON_AVAILABLE:\n",
    "        print(\"\\nüìä Agent Workflow Graph:\")\n",
    "        display(graph_viz)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Graph visualization not available: {e}\")\n",
    "\n",
    "print(f\"\\nüîç Extracting clauses...\")\n",
    "clause_start = time.time()\n",
    "clause_result = clause_agent.extract_clauses(document_id=document_id)\n",
    "clause_time = time.time() - clause_start\n",
    "\n",
    "print(f\"\\n‚úÖ Complete! ({clause_time:.2f}s)\")\n",
    "print(f\"   Clauses: {len(clause_result.clauses)}, Red Flags: {len(clause_result.red_flags)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f109b0",
   "metadata": {},
   "source": [
    "## 4.2: Risk Scoring Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1484bd",
   "metadata": {
    "title": "Risk Scoring Agent"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RISK SCORING AGENT\n",
      "================================================================================\n",
      "\n",
      "üéØ Scoring risks...\n",
      "\n",
      "‚úÖ Complete! (23.46s)\n",
      "   Overall Risk: 65/100 (High)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.agents.risk_scoring import RiskScoringAgent\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RISK SCORING AGENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "risk_agent = RiskScoringAgent()\n",
    "\n",
    "print(f\"\\nüéØ Scoring risks...\")\n",
    "risk_start = time.time()\n",
    "risk_result = risk_agent.score_risks(\n",
    "    clause_extraction_result=clause_result,\n",
    "    document_id=document_id\n",
    ")\n",
    "risk_time = time.time() - risk_start\n",
    "\n",
    "print(f\"\\n‚úÖ Complete! ({risk_time:.2f}s)\")\n",
    "print(f\"   Overall Risk: {risk_result.overall_risk_score}/100 ({risk_result.overall_risk_category})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a740b",
   "metadata": {},
   "source": [
    "## 4.3: Summary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38c6d17",
   "metadata": {
    "title": "Summary Agent"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY AGENT\n",
      "================================================================================\n",
      "\n",
      "üìù Generating diligence memo...\n",
      "\n",
      "‚úÖ Complete! (34.27s)\n",
      "   Findings: 1, Recommendations: 6\n",
      "\n",
      "================================================================================\n",
      "EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "M&A document analysis with overall risk score of 65/100\n",
      "\n",
      "Risk: High risk (65/100)\n",
      "\n",
      "üíæ Memo saved to: diligence_memo_combined.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.agents.summary import SummaryAgent\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY AGENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "summary_agent = SummaryAgent(retriever=retriever)\n",
    "\n",
    "print(f\"\\nüìù Generating diligence memo...\")\n",
    "summary_start = time.time()\n",
    "memo = summary_agent.generate_summary(\n",
    "    risk_scoring_result=risk_result,\n",
    "    document_id=document_id\n",
    ")\n",
    "summary_time = time.time() - summary_start\n",
    "\n",
    "print(f\"\\n‚úÖ Complete! ({summary_time:.2f}s)\")\n",
    "print(f\"   Findings: {len(memo.key_findings)}, Recommendations: {len(memo.recommendations)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{memo.executive_summary.overview}\")\n",
    "print(f\"\\nRisk: {memo.executive_summary.overall_risk_assessment}\\n\")\n",
    "\n",
    "# Export memo\n",
    "memo_md = memo.to_markdown()\n",
    "memo_path = backend_path / \"diligence_memo_combined.md\"\n",
    "with open(memo_path, \"w\") as f:\n",
    "    f.write(memo_md)\n",
    "print(f\"üíæ Memo saved to: {memo_path.name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d6862",
   "metadata": {},
   "source": [
    "## 4.4: Source Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915f3747",
   "metadata": {
    "title": "Source Tracker"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SOURCE TRACKER\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Created 3 source references\n",
      "üîó Generated 3 frontend links\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.utils.source_tracker import SourceTracker\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SOURCE TRACKER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tracker = SourceTracker(document_id=document_id)\n",
    "sample_chunks = vector_store.as_retriever(search_kwargs={\"k\": 3}).invoke(\"payment terms\")\n",
    "\n",
    "source_refs = []\n",
    "for i, chunk in enumerate(sample_chunks[:3], 1):\n",
    "    page = chunk.metadata.get(\"page\") if hasattr(chunk, 'metadata') else None\n",
    "    source_ref = tracker.create_source_reference(\n",
    "        chunk_id=f\"chunk_{i}\",\n",
    "        text_snippet=chunk.page_content[:150],\n",
    "        page=page,\n",
    "        confidence=0.95\n",
    "    )\n",
    "    source_refs.append(source_ref)\n",
    "\n",
    "source_metadata = tracker.create_source_metadata(\n",
    "    sources=source_refs,\n",
    "    extraction_method=\"llm_extraction\"\n",
    ")\n",
    "\n",
    "links = tracker.generate_links(source_metadata)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(source_refs)} source references\")\n",
    "print(f\"üîó Generated {len(links)} frontend links\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c0460",
   "metadata": {},
   "source": [
    "## 4.5: Checklist Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda075fc",
   "metadata": {
    "title": "Checklist Agent"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:app.agents.checklist:ChecklistAgent initialized with model: gpt-4o-mini\n",
      "INFO:app.agents.checklist:Starting checklist generation\n",
      "INFO:app.agents.checklist:Invoking ReAct agent for checklist generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKLIST AGENT\n",
      "================================================================================\n",
      "\n",
      "üìã Generating checklist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Legal\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Financial\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Operational\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Risk Management\n",
      "INFO:app.agents.checklist:Generated 7 risk-based checklist items\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.checklist:Generated 1 follow-up questions\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.checklist:Parsed 27 checklist items and 1 questions from agent messages\n",
      "INFO:app.agents.checklist:Checklist generation complete: 27 items, 1 questions in 17.90s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Complete! (17.90s)\n",
      "   Items: 27, Questions: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.agents.checklist import ChecklistAgent\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHECKLIST AGENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checklist_agent = ChecklistAgent(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "print(f\"\\nüìã Generating checklist...\")\n",
    "checklist_start = time.time()\n",
    "checklist_result = checklist_agent.generate_checklist(memo)\n",
    "checklist_time = time.time() - checklist_start\n",
    "\n",
    "print(f\"\\n‚úÖ Complete! ({checklist_time:.2f}s)\")\n",
    "print(f\"   Items: {len(checklist_result.checklist_items)}, Questions: {len(checklist_result.follow_up_questions)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333486a7",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: End-to-End Orchestration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248f5c92",
   "metadata": {
    "title": "Orchestration"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:app.rag.vector_store:Initializing in-memory Qdrant client\n",
      "WARNING:app.rag.chunking:SemanticChunker from langchain_experimental not available. Using sentence-based fallback implementation.\n",
      "INFO:app.rag.chunking:Initialized SemanticChunker (using_langchain=False) with threshold_type=percentile, threshold_amount=95.0, embedding_model=text-embedding-3-small\n",
      "INFO:app.pipelines.ingestion_pipeline:IngestionPipeline initialized with shared vector store\n",
      "INFO:app.agents.clause_extraction:ClauseExtractionAgent initialized with create_react_agent (lazy retriever)\n",
      "INFO:app.agents.risk_scoring:RiskScoringAgent initialized with create_react_agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "END-TO-END ORCHESTRATION\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:app.agents.summary:SummaryAgent initialized with create_react_agent\n",
      "INFO:app.agents.checklist:ChecklistAgent initialized with model: gpt-4o-mini\n",
      "INFO:app.orchestration.pipeline:DocumentOrchestrator initialized with model=gpt-4o-mini\n",
      "INFO:app.orchestration.pipeline:Starting orchestration for document: /Users/rbblankson34/Documents/Projects/Legal_AI/Legal-OS/data/Freedom_Final_Asset_Agreement.pdf\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=start, completed=[]\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=ingestion_agent\n",
      "INFO:app.orchestration.pipeline:Routing to: ingestion_agent\n",
      "INFO:app.orchestration.pipeline:Executing ingestion agent\n",
      "INFO:app.pipelines.ingestion_pipeline:Starting ingestion for: /Users/rbblankson34/Documents/Projects/Legal_AI/Legal-OS/data/Freedom_Final_Asset_Agreement.pdf\n",
      "INFO:app.pipelines.ingestion_pipeline:Loading document: /Users/rbblankson34/Documents/Projects/Legal_AI/Legal-OS/data/Freedom_Final_Asset_Agreement.pdf\n",
      "INFO:app.pipelines.ingestion_pipeline:Successfully loaded document Freedom_Final_Asset_Agreement.pdf (257988 chars, 84 pages/sections)\n",
      "INFO:app.rag.chunking:Chunking document dbace382-7788-43aa-96db-2deebff20be5 (length: 257988 chars)\n",
      "INFO:app.rag.chunking:Created 345 chunks for document dbace382-7788-43aa-96db-2deebff20be5\n",
      "INFO:app.rag.chunking:Successfully created 345 chunks for document dbace382-7788-43aa-96db-2deebff20be5\n",
      "INFO:app.rag.vector_store:Creating vectorstore from 345 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running complete pipeline (this may take 2-4 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:app.rag.vector_store:Vectorstore created with 345 documents\n",
      "INFO:app.pipelines.ingestion_pipeline:Successfully ingested Freedom_Final_Asset_Agreement.pdf: 345 chunks stored\n",
      "INFO:app.orchestration.pipeline:Ingestion complete: 345 chunks created\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=clause_extraction, completed=['ingestion']\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=clause_extraction_agent\n",
      "INFO:app.orchestration.pipeline:Routing to: clause_extraction_agent\n",
      "INFO:app.orchestration.pipeline:Executing clause extraction agent\n",
      "INFO:app.agents.clause_extraction:Starting clause extraction for document_id=freedom_asset_agreement\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.rag.retrievers:Creating naive retriever with k=10\n",
      "INFO:app.rag.retrievers:Creating naive retriever with k=10\n",
      "INFO:app.rag.retrievers:Creating naive retriever with k=10\n",
      "INFO:app.rag.vector_store:Creating retriever with search_kwargs: {'k': 10}\n",
      "INFO:app.rag.vector_store:Creating retriever with search_kwargs: {'k': 10}\n",
      "INFO:app.rag.vector_store:Creating retriever with search_kwargs: {'k': 10}\n",
      "INFO:app.agents.clause_extraction:Retriever created lazily with top_k=10\n",
      "INFO:app.agents.clause_extraction:Retriever created lazily with top_k=10\n",
      "INFO:app.agents.clause_extraction:Retriever created lazily with top_k=10\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.clause_extraction:Agent completed with 14 messages\n",
      "INFO:app.orchestration.pipeline:Clause extraction complete: 3 clauses, 3 red flags\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=risk_scoring, completed=['ingestion', 'clause_extraction']\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=risk_scoring_agent\n",
      "INFO:app.orchestration.pipeline:Routing to: risk_scoring_agent\n",
      "INFO:app.orchestration.pipeline:Executing risk scoring agent\n",
      "INFO:app.agents.risk_scoring:Starting risk scoring for document_id=freedom_asset_agreement\n",
      "INFO:app.agents.risk_scoring:Scoring 3 clauses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found clause: Payment Terms\n",
      "‚úÖ Found clause: Indemnification\n",
      "‚úÖ Found clause: warranties\n",
      "‚úÖ Found red flag: High\n",
      "‚úÖ Found red flag: Medium\n",
      "‚úÖ Found red flag: Critical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.risk_scoring:Agent completed with 17 messages\n",
      "INFO:app.agents.risk_scoring:‚úÖ Scored Payment Terms: 65 (High)\n",
      "INFO:app.agents.risk_scoring:‚úÖ Scored Indemnification: 65 (High)\n",
      "INFO:app.agents.risk_scoring:‚úÖ Scored warranties: 65 (High)\n",
      "INFO:app.orchestration.pipeline:Risk scoring complete: 3 clauses scored, overall risk=65\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=summary, completed=['ingestion', 'clause_extraction', 'risk_scoring']\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=summary_agent\n",
      "INFO:app.orchestration.pipeline:Routing to: summary_agent\n",
      "INFO:app.orchestration.pipeline:Executing summary agent\n",
      "INFO:app.agents.summary:Starting summary generation for document_id=freedom_asset_agreement\n",
      "INFO:app.agents.summary:Processing 3 scored clauses\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.summary:Agent completed with 10 messages\n",
      "INFO:app.agents.summary:‚úÖ Summary generated: 3 clause summaries, 1 findings, 0 recommendations\n",
      "INFO:app.orchestration.pipeline:Summary complete: 1 findings, 0 recommendations\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=provenance, completed=['ingestion', 'clause_extraction', 'risk_scoring', 'summary']\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=provenance_agent\n",
      "INFO:app.orchestration.pipeline:Routing to: provenance_agent\n",
      "INFO:app.orchestration.pipeline:Executing provenance agent\n",
      "INFO:app.utils.source_tracker:SourceTracker initialized for document_id=freedom_asset_agreement\n",
      "INFO:app.orchestration.pipeline:Provenance tracking complete: 3 items tracked\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=checklist, completed=['ingestion', 'clause_extraction', 'risk_scoring', 'summary', 'provenance']\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=checklist_agent\n",
      "INFO:app.orchestration.pipeline:Routing to: checklist_agent\n",
      "INFO:app.orchestration.pipeline:Executing checklist agent\n",
      "INFO:app.agents.checklist:Starting checklist generation\n",
      "INFO:app.agents.checklist:Invoking ReAct agent for checklist generation\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Legal\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Operational\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Risk Management\n",
      "INFO:app.agents.checklist:Generated 5 standard checklist items for category: Financial\n",
      "INFO:app.agents.checklist:Generated 1 risk-based checklist items\n",
      "INFO:app.agents.checklist:Generated 1 follow-up questions\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:app.agents.checklist:Parsed 21 checklist items and 1 questions from agent messages\n",
      "INFO:app.agents.checklist:Checklist generation complete: 21 items, 1 questions in 14.85s\n",
      "INFO:app.orchestration.pipeline:Checklist complete: 21 questions generated\n",
      "INFO:app.orchestration.pipeline:Supervisor analyzing state: current_step=complete, completed=['ingestion', 'clause_extraction', 'risk_scoring', 'summary', 'provenance', 'checklist']\n",
      "INFO:app.orchestration.pipeline:Supervisor decision: next_agent=__end__\n",
      "INFO:app.orchestration.pipeline:Routing to: __end__\n",
      "INFO:app.orchestration.pipeline:Orchestration complete: 6 steps completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Orchestration Complete! (142.53s)\n",
      "   Status: completed\n",
      "   Steps: 6/6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.orchestration.pipeline import DocumentOrchestrator\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"END-TO-END ORCHESTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "orchestrator = DocumentOrchestrator(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "print(f\"\\nüöÄ Running complete pipeline (this may take 2-4 minutes)...\")\n",
    "orch_start = time.time()\n",
    "orch_results = orchestrator.run_orchestration(\n",
    "    document_path=str(sample_doc),\n",
    "    document_id=\"freedom_asset_agreement\"\n",
    ")\n",
    "orch_time = time.time() - orch_start\n",
    "\n",
    "print(f\"\\n‚úÖ Orchestration Complete! ({orch_time:.2f}s)\")\n",
    "print(f\"   Status: {orch_results['status']}\")\n",
    "print(f\"   Steps: {len(orch_results['completed_steps'])}/6\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242120f",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Results & Export\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33278d13",
   "metadata": {
    "title": "Results Summary"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate total time for sequential workflow\n",
    "sequential_time = (ingestion_time + rag_time + clause_time + risk_time + \n",
    "                   summary_time + checklist_time)\n",
    "\n",
    "print(f\"\\nüìä Sequential Workflow:\")\n",
    "print(f\"   Total Time: {sequential_time:.2f}s\")\n",
    "print(f\"   - Ingestion: {ingestion_time:.2f}s\")\n",
    "print(f\"   - RAG Test: {rag_time:.2f}s\")\n",
    "print(f\"   - Clause Extraction: {clause_time:.2f}s\")\n",
    "print(f\"   - Risk Scoring: {risk_time:.2f}s\")\n",
    "print(f\"   - Summary: {summary_time:.2f}s\")\n",
    "print(f\"   - Checklist: {checklist_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nüìä Orchestrated Workflow:\")\n",
    "print(f\"   Total Time: {orch_time:.2f}s\")\n",
    "print(f\"   Efficiency: {((sequential_time - orch_time) / sequential_time * 100):.1f}% faster\")\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Clauses Extracted: {len(clause_result.clauses)}\")\n",
    "print(f\"   Red Flags: {len(clause_result.red_flags)}\")\n",
    "print(f\"   Overall Risk: {risk_result.overall_risk_score}/100\")\n",
    "print(f\"   Findings: {len(memo.key_findings)}\")\n",
    "print(f\"   Recommendations: {len(memo.recommendations)}\")\n",
    "print(f\"   Checklist Items: {len(checklist_result.checklist_items)}\")\n",
    "\n",
    "# Export consolidated results\n",
    "export_data = {\n",
    "    \"document_id\": document_id,\n",
    "    \"document_name\": sample_doc.name,\n",
    "    \"processing_times\": {\n",
    "        \"sequential_total\": sequential_time,\n",
    "        \"orchestrated_total\": orch_time,\n",
    "        \"ingestion\": ingestion_time,\n",
    "        \"clause_extraction\": clause_time,\n",
    "        \"risk_scoring\": risk_time,\n",
    "        \"summary\": summary_time,\n",
    "        \"checklist\": checklist_time\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"clauses_extracted\": len(clause_result.clauses),\n",
    "        \"red_flags\": len(clause_result.red_flags),\n",
    "        \"overall_risk_score\": risk_result.overall_risk_score,\n",
    "        \"overall_risk_category\": risk_result.overall_risk_category,\n",
    "        \"findings\": len(memo.key_findings),\n",
    "        \"recommendations\": len(memo.recommendations),\n",
    "        \"checklist_items\": len(checklist_result.checklist_items),\n",
    "        \"follow_up_questions\": len(checklist_result.follow_up_questions)\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = backend_path / \"combined_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Consolidated results saved to: {results_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ LEGAL-OS DEMONSTRATION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(f\"   - {memo_path.name} (Diligence memo)\")\n",
    "print(f\"   - {results_path.name} (Consolidated results)\")\n",
    "print(\"\\nKey Achievements:\")\n",
    "print(\"   ‚úÖ Document ingestion with shared vector store\")\n",
    "print(\"   ‚úÖ RAG pipeline with question answering\")\n",
    "print(\"   ‚úÖ Clause extraction with red flag detection\")\n",
    "print(\"   ‚úÖ Risk scoring with categorization\")\n",
    "print(\"   ‚úÖ Diligence memo generation\")\n",
    "print(\"   ‚úÖ Source provenance tracking\")\n",
    "print(\"   ‚úÖ Checklist and follow-up questions\")\n",
    "print(\"   ‚úÖ End-to-end orchestration\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
